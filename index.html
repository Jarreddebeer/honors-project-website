<html lang="en">
<head>
    <link rel="stylesheet" href="./css/bootstrap.min.css" />
    <link rel="stylesheet" href="./css/github-markdown.css" />
    <link rel="stylesheet" href="./css/lightbox.css" />
    <link rel="stylesheet" href="./css/style.css" />
    <link rel="stylesheet" href="./css/font-awesome.min.css">
</head>

<div class="app-wrapper">

    <header class="navbar navbar-default" id="top" role="banner">
      <div class="container">
        <div class="navbar-header">
          <a href="/" class="navbar-brand">GPUStar</a>
        </div>
        <nav>
          <ul class="nav navbar-nav navbar-right">
              <li><a href="#overview">Overview</a></li>
              <li><a href="#results">Results</a></li>
              <li><a href="#writeup">Writeup</a></li>
              <li><a href="#downloads">Downloads</a></li>
          </ul>
        </nav>
      </div>
    </header>

<div id="introduction">

        <div class="blocks">
            <div class="block-container">
                <div class="intro-text clearfix">
                    <div id="by">
                        <h1>Accelerating Automated Extraction of Radio Astronomical Sources from Observation Data with GPU Accelerators</h1>
                        <span class="students">by Yaseen Hamdulay and Jarred de Beer</span>
                        <span class="supervised">supervised by Assoc. Prof Michelle Kuttel and Dr Sarah Blyth</span>
                    </div>
                    <div id="objective">
                        <img src="./img/spiral-galaxy-icon.svg" class="spiral" />
                        <h1>GPUSTAR</h1>
                        <p>Next generation radio interferometers such as ASKAP and MeerKAT are under development to detect HI spectral emissions from spiral galaxies and celestial bodies in an attempt to help scientists understand the cosmic neutral gas density of the universe and its evolution. 'Meer' is Afrikaans for 'more' indicating that more telescopes will be used, generating Petabytes of data. Traditionally, detection of HI spectral emissions has been a manual process but now even automated solutions need to be accelerated.
                        In this project we adopt the Assess-Parallelise-Optimise-Deploy methodology from CUDA's Best Practices handbook to iteratively develop CUDA kernels and achieve incremental speedups for the A'Trous wavelet reconstruction from DUCHAMP and Smooth-and-Clip from SoFiA.</p>
                    </div>
                </div>
            </div>
        </div>

        <svg class="starfield far a"></svg>
        <svg class="starfield far b"></svg>

        <svg class="starfield medium a"></svg>
        <svg class="starfield medium b"></svg>

        <svg class="starfield nearby a"></svg>
        <svg class="starfield nearby b"></svg>

        <svg class="lightspeed-starfield nearby a"></svg>
        <svg class="lightspeed-starfield nearby b"></svg>
</div>

<div id="main">

    <div class="row" id="overview">

        <div class="duchamp col-sm-5">
            <div id="report-target-duchamp" class="col-sm-12 report">
                <h1>DUCHAMP</h1>
                <p>Yaseen's intro piece from poster</p>
            </div>
        </div>

        <div class="center-column col-sm-2">
            <img id="antenna" src="./img/satelite-icon.svg" />
            <img id="tower" src="./img/computer-tower-icon.svg" />
        </div>

        <div class="sofia col-sm-5">

            <div id="report-target-sofia" class="col-sm-12 report">
                <h1>SoFiA</h1>
                <p>SoFiA is in development for various ASKAP surveys and to search for emissions on multiple scales while considering variations in noise level. The Smooth-and-Clip (S+C) implementation in SoFiA is a computationally expensive process which performs a Gaussian filter along the Y-axis, then again along the X-axis, and lastly either a uniform or a gaussian filter along the Z-axis. Pixel intensities from the smoothed cube are then thresholded to identify sources. SoFiA is single threaded and written in Python. We utilise CFFI to offload the filtering processes to an equivalent C module, passing a pointer to the underlying data cube which is to be filtered. The filtering logic then transfers the data in portions onto the GPU where its filtered before being transferred back to the host in-place. Two CUDA kernels are used which perform the Gaussian and Uniform filtering.</p>
            </div>
        </div>
    </div>

    <div class="row cuda-header" id="results">
        <div class="col-sm-5">
            <div class="speedup-graph">
                <img src="./img/duchamp-accelerated-results.jpg" />
            </div>
        </div>
        <div class="col-sm-2 center-column">
            <h3>Results</h3>
            <img src="./img/cpu-icon.svg" />
            <img src="./img/gpu-icon.svg" />
        </div>
        <div class="col-sm-5">
            <div class="speedup-graph">
                <img src="./img/sofia-accelerated-results.png" />
            </div>
        </div>
    </div>

</div> <!-- main -->

</div> <!-- app-wrapper -->
<div class="app-wrapper light">


    <div class="row" id="writeup">
        <div class="col-sm-6">

            <div class="speedup-graph quote">
                <p><i class="fa fa-quote-left"></i>Yaseen's conclusion from poster<i class="fa fa-quote-right"></i></p>
            </div>

            <h3>Yaseens content</h3>

        </div>
        <div class="col-sm-6">

            <div class="speedup-graph quote">
                <p><i class="fa fa-quote-left"></i>S+C acceleration increased with cube size from 2.88x on a 327MB data cube to 8.6x on a 2.3GB data cube. Execution time reduced from 535 seconds to 70 seconds, a 7.6x speedup which makes GPU processing favorable.<i class="fa fa-quote-right"></i></p>
            </div>

            <h3>Introduction</h3>

            <p>The SoFiA source finding framework is being developed in preparation for various ASKAP surveys: WALLABY, a wide survey covering 3/4 of the sky at z-0.25; DINGO, a deep survey reaching z-0.4; and APERTIF.
               It is intended to search for emissions on multiple scales while also considering variations in noise level. Three source finding algorithms are implemented: Simple threshold, Smooth-and-Clip (S+C), and Characterised noise HI (CNHI). SoFiA's modularity allows additional algorithms to be integrated.
               Data is stored in the 3D Flexible Image Transport System (FITS) format, adequate for MeerKAT and ASKAP surveys which will generate spectral data cube sizes of up to 2.5 Terabytes. The 3D data is represented by two spatial dimensions which map to coordinates in the sky and one spectral dimension which maps to the spectral frequency of HI detections.</p>

            <p>Intensity threshold source finders such as S+C compare pixel values against an absolute threshold to classify source pixels. An inherent limitation is a decreased contribution of total flux in higher resolution data cubes as sources are distributed into a larger number of pixels.</p>

            <p>SoFiA's pipeline consists of five stages: First, data is input and modified according to flags or weights; Second, filters are applied to reduce noise; Third, source detection which employs various source detection algorithms; Fourth, sources are merged and parametrised; Fifth, the results and binary mask containing sources are output.</p>

            <p>Most source finding packages and algorithms execute sequentially and are CPU bound. The Scalable Source Finding Framework (SSoFF) by Westerlund et al. has been developed to distribute workload among a grid based cluster of nodes, but source finders need to be implemented individually. The SSoFF implementation of the Parallel Gaussian source finder (PGSF) is such an example. PGSF was later implemented on the Graphics Processing Unit (GPU) by Westerlund et al. who report significant speedups over the CPU. Similar GPU results have been reported by Huang et al. of a 25x speedup and 9x less energy consumption than multi-threaded CPU implementations.</p>

            <p>The favorable performance and energy consumption of GPU processing makes it an attractive technology for use in radio astronomy surveys such as MeerKAT and ASKAP. We aim to accelerate the S+C implementation in SoFiA on the GPU using CUDA, and are interested in the amount of speedup and computational throughput obtainable. We hope to decrease astronomer's time-to-result when using the SoFiA package and increase the throughput of ASKAP and MeerKAT surveys.</p>

            <h3>The Smooth-and-clip algorithm</h3>

            <p>The Smooth-and-Clip (S+C) algorithm in SoFiA performs a 1D filter convolution in a specific order along each individual axis. A Gaussian filter is first applied along the Y-axis, then again along the X-axis, and lastly either a uniform or a gaussian filter is applied along the Z-axis. After the data cube has been smoothed the pixel values are fitted against a Gaussian distribution from which the Root Mean Square (RMS) is calculated. The RMS value is the intensity threshold to add pixels to the mask. SoFiA uses the Gaussian and Uniform filter provided by scipy's image processing module. scipy is a python library with calls to C and Fortran code optimised for compute.</p>

            <h3>Design</h3>

            <p>We aim to replace CPU intensive filtering from S+C with CUDA implementations in C to be processed on the GPU. The implementation is designed to be integrated back into SoFiA with minimal overhead to its existing pipeline.</p>

            <p>We design for individual compute nodes equipped with a CUDA enabled GPU device. When data exceeds device memory, the data cube will be split and processed in parts on the GPU. Compute nodes contain at least 4GB of memory to process data cubes of up to 2GB. Data cubes are in the Flexible Image Transport System (FITS) file format. The implementation is developed against version 0.4.0 of SoFiA's github repository. SoFiA's default parameter set for S+C will be supported and fall back gracefully to original serial execution where either the control flow has not been implemented for the input parameters or no CUDA enabled device is detected.</p>

            <h4>Approach</h4>

            <p>In the first pass, the execution time of the S+C algorithm is analysed to detect slow sub-routines, which are re-implemented with a serial-C library with matching API. Test cases for each implementation verify output against the original version. In the second pass we parallelise the serial-C version in two phases: First with a multi-threaded OpenMP implementation and second with a CUDA implementation.</p>

            <h4>System Architecture</h4>

            <p>SoFiA's original architecture remains unchanged: Our implementation integrates with SoFiA's pipeline. Here, the left column represents the S+C pipeline written in Python and the right column represents our implementation written in C with its CUDA kernels. The data cube is processed in place and in parts on the host using device memory.</p>
            <p class="centered"><a href="./img/sofia/sofia-architecture.jpg" data-lightbox="sofia-architecture" data-title="SoFiA architecture" /><img src="./img/sofia/sofia-architecture.jpg" class="img-thumbnail" /></a></p>

            <h4>Hardware</h4>

            <p>Computations were performed using facilities provided by the University of Cape Town's ICTS High Performance Computing team: <a href="http://hpc.uct.ac.za" target="_blank">http://hpc.uct.ac.za</a>. Execution time is evaluated on hardware provided in table</p>
            <p class="centered"><a href="./img/sofia/sofia-hardware.jpg" data-lightbox="sofia-hardware" data-title="Hardware" /><img src="./img/sofia/sofia-hardware.jpg" class="img-thumbnail full" /></a></p>

            <h3>Profiling Smooth-and-Clip</h3>

            <p>S+C execution appears linear with respect to cube size and global RMS calculation adds nearly 75% onto the overall execution time. The kernel sizes increase every 4th kernel in X and Y to 0, 3 and 6 respectively which explains the stepped effect. Kernel sizes in Z cycle between 0, 3, 7, 15 yet appear constant in 3, 7 and 15 because the Uniform implementation is independent of kernel size.</p>
            <p class="centered"><a href="./img/sofia/sofia-evaluation-s+c.jpg" data-lightbox="sofia-evaluation-s+c" data-title="S+C Evaluation" /><img src="./img/sofia/sofia-evaluation-s+c.jpg" class="img-thumbnail full" /></a></p>

            <h4>RMS</h4>
            <p>Global RMS is responsible for 99% of total RMS time on the 2.3GB cube and contributes 38.8% to the combined execution time. RMS during filtering contributes 0.05% towards filtering time and is unaffected by kernel size. We conclude that accelerating RMS is worthwhile given global RMS execution time.</p>

            <h4>Filtering</h4>
            <p>Filtering contributes 61.1% of the reduced pipeline's execution time, with the Gaussian and Uniform filters contributing almost entirely towards filtering. Gaussian filtering, applied to both X and Y axes, increases with kernel size. Uniform filtering, applied to the Z axis, is unaffected by kernel size. In total filtering consumes 327 seconds, with 150 seconds for Gaussian filtering and 174 seconds for Uniform filtering for the 2.3GB file.</p>

            <h4>CUDA Evaluation</h4>
            <p>S+C is evaluated with the APOD development methodology provided by the CUDA Best Practices Handbook. Our primary focus is CUDA acceleration and we do not optimise for the serial-C and OpenMP implementations. Assessments to CUDA implementations are incremental and calculate the speedup against the original python execution time. S+C is implemented by the <i>SCfinder_mem</i> method from <i>pyfind.py</i> in the <i>sofia</i> module. We evaluate <i>SCfinder_mem</i>'s execution time as the ultimate acceleration metric achieved for the project.</p>

            <h3>Implementation</h3>
            <p>The implementation, although focused on acceleration, is designed to be compatible with the SoFiA package. This is done to maximise the likelihood of integrating the changes back into SoFiA. We make no adjustments to the SoFiA pipeline itself, instead we alter the <i>SCfinder_mem</i> sub routine by replacing the filtering processes with a single call to our own python method <i>C_SCfinder_mem</i> which takes the copy of the data cube and a kernel as input. This is a wrapper method which uses CFFI to call to our C implementation and performs the same filtering logic as the replaced lines but instead using CUDA for filtering.</p>

            <h4>Gaussian filter implementation</h4>
            <p>Gaussian performs separable convolution along X and Y axes, which have the least stride in memory alignment. Our Gaussian implementation therefore subdivides the cube along the Z-axis, ensuring X-Y planes of data fit onto the device in their entirety. This has three advantages: A simpler kernel which does not need to handle padding within the planes themselves, but only at their borders with zeros; both X and Y filtering is performed per plane with a single copy to the device; and this favors the typical cube structure which are longer along the spectral Z-axis. However, it is unable to split X-Y planes and cannot process cubes where a single X-Y plane is too large to fit onto the device. We utilise shared memory to minimise latency from memory access since each pixel samples multiple neighbouring pixels. The gaussian weight values are calculated once on the CPU and cached in constant memory on the device for low latency access. We tested a range of CUDA kernels to find an optimal implementation.</p>
            <p class="centered"><a href="./img/sofia/sofia-evaluation-gaussian.jpg" data-lightbox="sofia-evaluation-gaussian" data-title="Gaussian evaluation" /><img src="./img/sofia/sofia-evaluation-gaussian.jpg" class="img-thumbnail full" /></a></p>

            <h4>Uniform filter implementation</h4>
            <p>The Uniform filter performs a 1D convolution along the Z axis, which has the biggest stride in memory alignment. Fortunately we take advantage of a sliding average implementation which amortises the memory accesses required in the initial average calculation and remaining accesses to 1. This implementation starts at the first Z-index and processes pixels along Z until the end pixel is reached. Generally the matrix is transposed beforehand to coalesce pixels along the Z-axis which results in a significant performance improvement. We did not implement this as it requires a duplicate cube and this would break our limited memory constraint and we did not attempt an in-place matrix transposition with the GPU. Data is subdivided into sections by the Z-axis in the same manner as the Gaussian filter except in this case pixel values are required for padding. Sliding averages are calculated at the beginning of each Z-index per section. Each Z-index is assigned to a single thread. A disadvantage of this is the number of averages calculated per Z-index increases by the number of sections. Each pixel is accessed only once so we would not benefit from shared memory and the implementation just uses global memory.</p>
            <p>We note that the CUDA implementation crashes abruptly when there is insufficient device memory and it is difficult to predict how this will happen. We could not allocate all available memory without experiencing a crash and as a result shave off 15MB from the reported available memory, but we do not know of an optimal value.</p>

            <h3>Results and discussion</h3>
            <p>We first detail speedups for the Gaussian and Uniform filters and RMS, then list final speedups on S+C execution time. The Gaussian code ran on data cubes with a fixed height of 320px and varied square, planar width. The Uniform code ran on data cubes with a fixed square, planar width of 320px and varying height. The S+C results ran on the 327MB, 724MB and 2.3GB FITS files. CUDA results are discussed in their order of implementation using the APOD approach.</p>

            <h4>Gaussian filter</h4>
            <p>The original CPU version runs in quadratic time with respect to cube size and increases with respect to increasing filter size. The spike is a result of paged memory alignment. Here the data cube with square width of 512 results in memory accesses aligned to the same line in cache. This causes frequent L1 and L2 cache in-validations from paged memory alignments. Our CUDA implementation did not experience the same effect and the result is a dramatically increased execution time on the CPU and comparatively high speedups that obscure results. Cube sizes therefore avoid powers of 2 but are detailed separately.</p>
            <p>Our serial-C implementation is much slower than the original scipy version, which is heavily optimised for speed. However, our implementation uses less memory at 2 times the original cube size whereas the SoFiA implementation peaks between 3 and 4 times the cube size. The OpenMP implementation performed up to 1.5x faster than the original scipy version on the quad-core i3 CPU.</p>
            <p>Our naive, unoptimised CUDA implementation on a 430 GT device using Doubles, shared memory and a block size of 1024 threads (32x32) shows constant speedup or around 1.5x on all cube sizes. The dip in performance for small cubes is likely due to overhead in copying data between device and host. For Doubles, filter sizes of 15 could not be computed on the 430 GT device due to limited memory. A kernel of size 12 is used instead.</p>
            <p>The naive implementation is first improved for for memory access and second for compute time, since memory latency is the biggest contributing factor to GPU performance. Due to time constraints we did not attempt low priority optimisations specified in the <i>CUDA Best Practices Handbook</i>. The first improvement aligns Y-axis pixels in shared memory for processing on the Y-axis. The improvement is minimal and constant, with larger gains achieved by larger filters due to the increased coherent accesses.</p>
            <p>The implementation is then optimised to use Floats instead of Doubles. This affects both memory access and compute time as the memory footprint is halved, allowing twice the number of pixels to be processed per interval by the GPU. The GPU has more floating point arithmetic units than double floating point arithmetic units and the extra usage results in higher occupancy. The improvement is more pronounced at larger cube sizes that exceed device memory and benefit from half the number of data transfers, but is difficult to see at the given scale.</p>
            <p>Next we optimise for compute time. The Nvidia Graphical Profiler reported that reducing the block size from 1024 to 256 threads increases the number of concurrently executing warps from 32 to 48, which increased the speedup from 2.5x to 3.5x.</p>
            <p>The graphical profiler also reported under utilisation of shared memory, which can increase from 4KB to around 8KB and maintain 48 concurrent warps. The number of pixels loaded into shared memory and convolved per thread is increased to 4. This amortises the cost of calculating pixel indexes and reduces the proportion of padding in shared memory, reducing redundant loads from global memory and increasing occupancy to nearly 100%. However, increases in occupancy values above 50% do not necessarily improve performance.</p>

            <p class="centered"><a href="./img/sofia/sofia-results-gaussian.jpg" data-lightbox="sofia-results-gaussian" data-title="Gaussian results" /><img src="./img/sofia/sofia-results-gaussian.jpg" class="img-thumbnail full" /></a></p>

            <p>The spike at 512px is due to frequent cache invalidation caused by memory alignments. No optimisations were implemented and the speedups are purely the result of cube widths a multiple of 64.</p>
            <p>Asynchronous memory copies and kernel calls execute concurrently on devices with multiple compute and copy engines. The 430 GT device has 1 copy and compute engine and did not increase in speedup. The visual profiler reported that kernels begin executing nicely after the first copy task completes but do not begin to copy back to the host until the last kernel has completed. Increasing the number of asynchronous memory transfers and kernel calls to 8 on the Tesla M2090 device utilises its 2 copy engines to hide memory latency even further and improve speedups to 20x on larger filters.</p>
            <p>The last two optimisations use the improved hardware on the Tesla M2090. The blocksize is increased from 256 to 1024 threads which achieves a slight performance gain that is more pronounced on larger cube sizes. Finally, the number of consecutive pixels processed per thread is increased from 4 to 10. This is the maximum we could achieve and may not be optimal, only improving the 15px filter.</p>
            <p>With large kernel sizes, this best implementation of the Gaussian kernel achieves a maximum of 23x speedup.</p>

            <h4>Uniform filter</h4>
            <p>Execution of the Uniform filter is linear with respect to cube size and independent of kernel size. This is due to the sliding window implementation, which amortises memory accesses to 1 per pixel.</p>
            <p>The OpenMP implementation performed poorly, with a speedup of about 1.1 on the desktop CPU for every kernel size except 1. The kernel with size 1 yielded a constant speedup of 20 for Doubles and 40 for Floats. We attribute the otherwise low speedup to large strides in memory access along the Z-axis.</p>

            <p class="centered"><a href="./img/sofia/sofia-results-uniform.jpg" data-lightbox="sofia-results-uniform" data-title="Uniform results" /><img src="./img/sofia/sofia-results-uniform.jpg" class="img-thumbnail full" /></a></p>

            <p>Our initial naive CUDA kernel uses Doubles and achieves a relatively constant speedup of around 6 on the 430 GT device. This implementation reaches a performance plateau at a cube height of 320px, which is likely the point at which global device memory becomes fully allocated and occupancy is maximised.</p>
            <p>As before, we first optimise for memory accesses and second for compute time. Using Floats instead of Doubles halves cube size and memory strides. The adjustment is trivial and doubles the speedup to around 12. Smaller filter sizes have slightly larger speedups than larger filter sizes, which is likely due to fewer strided memory accesses. The opposite effect is seen in Gaussian filter sizes.</p>
            <p>Speedups on cube sizes which are powers of 2 increase linearly. This is similar but not as obvious given the cube range. We do not experience spikes in performance as we do with the Gaussian filter but instead experience overall increased speedup of up to 20x. This is achieved with square widths of 256 pixels and varying height by increments of 64.</p>
            <p>The optimal Uniform filter kernel achieved similar speedups on the desktop machine and HEX, with a maximum speedup of 13.78x on data cube sizes which are not a power of 2.</p>

            <h4>RMS</h4>
            <p>As of v0.5.0 of SoFiA the RMS method has been improved and our efforts to accelerate it are no longer appropriate. However, we briefly report on the results achieved for this method. We found that 94% of the total RMS execution time is in the histogram binning process and a naive serial-C implementation was written to replace it, which reduced the execution time from 208 to 32 seconds on the 2.3GB cube, a 6.4x speedup. Further reductions are expected using OpenMP but we did not explore this as filtering remained the primary bottleneck.</p>

            <h4>Smooth-and-Clip speedup</h4>
            <p>S+C results are achieved after integrating the Gaussian and Uniform CUDA kernels and the serial-C histogram method. We ran S+C on the three FITS files and the results show a linear relationship between execution time and cube size.</p>
            <p>Kernel sizes only increase in X and Y every 4th kernel which results in the stepped behaviour. However, kernel sizes in Z cycle between 0, 3, 6 and 15 which results in the jittered behaviour clearly apparent. Gaussian filtering execution time reduced from 150 seconds down to 16, a speedup of 9x. Uniform filtering reduced from 174 seconds to 20, a speedup of 8.3x. Local RMS as mentioned previously reduced from 1.89 to 0.225 seconds, a speedup of 8.4x.</p>
            <p>Overall execution time is reduced by a factor of 7.6x from 535 seconds to 70 on a 2.3GB data cube. The original implementation has constant throughput of 4.3GB/s on all three data cubes, while throughput for the accelerated implementation is 16GB/s, 23GB/s and 32GB/s for the 327MB, 724MB and 2.3GB files respectively.</p>
            <p class="centered"><a href="./img/sofia/sofia-results-s+c.jpg" data-lightbox="sofia-results-s+c" data-title="S+C results" /><img src="./img/sofia/sofia-results-s+c.jpg" class="img-thumbnail full" /></a></p>

            <h4>Conclusions</h4>
            <p>We have accelerated the S+C implementation in the SoFiA source finding package as follows: The filtering logic using scipy's Gaussian and Uniform filters were replaced with a C method executing similar Gaussian and Uniform CUDA kernels. GPU device memory is used as a convenient buffer to avoid redundant memory use on the host. Gaussian filtering is performed on two axes, X and Y, while Uniform filtering is performed on Z. In addition the Uniform filter implementation performs fewer memory accesses independent of filter radius.</p>
            <p>Despite this the original Uniform filter took longer on the host, at 174 seconds, with the Gaussian filter taking 150 seconds on a 2.3GB data cube. Similarly, the Uniform CUDA implementation ran slower at 20 seconds as opposed to 16 seconds for the Gaussian CUDA implementation. The poorer performance is a result of the Z axis having the largest stride in memory access but may benefit from in-place matrix transformation to coalesce Z-axis pixels in memory. Progressive, incremental improvements in the Gaussian kernel allowed us to achieve speedups of over 20x for larger kernel sizes.</p>
            <p>The GPU is highly beneficial for data cubes whose size is a power of 2. This is a result of degraded performance on the CPU due to frequent cache in-validations from aligned memory accesses. The GPU does not experience this, and a data cube of 512x512x256 achieved a 26x speedup with a filter radius of 3px, while similar cube sizes only achieved a 5x speedup. Our implementation uses less memory at 2 times the original cube size whereas the SoFiA implementation peaks between 3 and 4 times the cube size.</p>
            <p>Overall, S+C acceleration increased with cube size from 2.88x on a 327MB data cube to 8.6x on a 2.3GB data cube. Execution time reduced from 327 seconds to 39 seconds using 2.3GB data. Combined with global RMS total execution time was reduced from 535 seconds to 70 seconds, a 7.6x speedup.</p>
            <p>Results show CUDA is favorable over the CPU for filtering processes in source finding algorithms such as S+C in SoFiA. However, S+C is a single source finding algorithm and individual effort is required per algorithm to accelerate entire general purpose source finding pipelines such as SoFiA, as is also the case with the SSoFF framework.</p>

            <h4>Future work</h4>
            <p>The CUDA Best Practices Handbook lists several libraries which are optimised for CUDA processing such as cuBLAS and cuFFT. These could be used to mitigate the manual effort required to implement CUDA kernels in general pipelines such as SoFiA. Additionally, pyCUDA could be tested instead of C or C++ CUDA implementations, as this would avoid the complexity of maintaining C/C++ code in addition to Python.</p>
            <p>S+C supports both Uniform and Gaussian filtering along the Z-axis (spectral axis). Our implementation only supports Uniform filtering along Z, and work needs to be done to modify the Gaussian filter to the filter Z in addition to X and Y.</p>
            <p>The Gaussian kernel requires that entire X-Y planes of data are copied onto the device. This limits the maximum width of the data cube that can be processed to the available memory on the device. In order to process these larger cubes they would have to be subdivided on the host or elsewhere before being processed. The kernel can be adjusted with some effort to process subsections of X-Y planes and accommodate the necessary padding this incurs. However, this may not be necessary as data cubes are longer along the spectral Z axis and not X-Y.</p>
            <p>The code and additional dependency on CFFI can be integrated back into the SoFiA package. This project uses v0.4.0 of SoFiA but at the time of writing v0.5.0 has been released and further versions are under development. As of v0.5.0 the SoFiA developers have been focusing on C++ re-factors to integrate OpenMP into the pipeline and our CUDA efforts should not conflict with theirs.  OpenMP re-factoring on their part will conveniently clear a pathway for CUDA integration, which is yet to be undertaken.</p>

        </div>
    </div>

    <div class="row" id="downloads">
        <div class="col-sm-12">
            <h3>Downloads</h3>
            <table class="table">
                  <thead>
                    <tr>
                      <th>#</th>
                      <th>Title</th>
                      <th>Author</th>
                      <th>Twitter</th>
                      <th>Github</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <th scope="row">1</th>
                      <td><a target="_blank" href="./downloads/paper-gpustar-duchamp.pdf">DUCHAMP paper</a></td>
                      <td><a target="_blank" href="./downloads/litreview-gpustar-duchamp.pdf">Literature review</a></td>
                      <td>Yaseen Hamdulay</td>
                      <td><a target="_blank" href="https://twitter.com/mrhamdulay"><i class="fa fa-twitter"></i></a></td>
                      <td><a target="_blank" href="https://github.com/MrHamdulay"><i class="fa fa-github"></i></a></td>
                    </tr>
                    <tr>
                      <th scope="row">2</th>
                      <td><a target="_blank" href="./downloads/paper-gpustar-sofia.pdf">SoFiA paper</a></td>
                      <td><a target="_blank" href="./downloads/litreview-gpustar-sofia.pdf">Literature review</a></td>
                      <td>Jarred de Beer</td>
                      <td><a target="_blank" href="https://twitter.com/jarrhead"><i class="fa fa-twitter"></i></a></td>
                      <td><a target="_blank" href="https://github.com/jarreddebeer"><i class="fa fa-github"></i></a></td>
                    </tr>
                  </tbody>
            </table>

        </div>
    </div>

</div>



<div id="footer">
    <div class="row">
        <div class="col-sm-4">
            <img src="./img/cslogo.png" />
        </div>
        <div class="col-sm-4">
            <p>Department of Computer Science<br />
            University of Cape Town<br />
            Private Bag X3. Rondebosch 7701<br />
            tel: 021 650 2663
            </p>
        </div>
        <div class="col-sm-4">
            <img src="./img/uct-logo.jpg" />
        </div>
    </div>
</div>



<script src="./js/jquery.min.js"></script>
<script src="./js/markdown.min.js"></script>
<script src="./js/lightbox.min.js"></script>
<script>

/* MARKDOWN */

var src_sofia = document.getElementById('report-source-sofia');
var tgt_sofia = document.getElementById('report-target-sofia');
// tgt_sofia.innerHTML = markdown.toHTML(src_sofia.textContent);

var src_duchamp = document.getElementById('report-source-duchamp');
var tgt_duchamp = document.getElementById('report-target-duchamp');
// tgt_duchamp.innerHTML = markdown.toHTML(src_duchamp.textContent);


/* GRAPHS */

/*
render_speedups(".sofia .speedup-graph");

function render_speedups(selector) {

        // Set the dimensions of the canvas / graph
    var margin = {top: 30, right: 20, bottom: 30, left: 50},
        width = 650 - margin.left - margin.right,
        height = 400 - margin.top - margin.bottom;

    // Set the ranges
    var x = d3.scale.linear().range([0, width]);
    var y = d3.scale.linear().range([height, 0]);

    // Define the axes
    var xAxis = d3.svg.axis().scale(x)
        .orient("bottom").ticks(5);

    var yAxis = d3.svg.axis().scale(y)
        .orient("left").ticks(5);

    // Define the line
    var valueline = d3.svg.line()
        .x(function(d) { return x(d.size); })
        .y(function(d) { return y(d.duration); });

    // Adds the svg canvas
    var svg = d3.select(selector)
        .append("svg")
            .attr("width", width + margin.left + margin.right)
            .attr("height", height + margin.top + margin.bottom)
        .append("g")
            .attr("transform",
                  "translate(" + margin.left + "," + margin.top + ")");

    // Get the data
    data_original = [
        {size:327,duration:72},
        {size:720,duration:163},
        {size:2300,duration:535},
    ];

    data_accelerated = [
        {size:327,duration:20},
        {size:720,duration:31},
        {size:2300,duration:70},
    ];


        // Scale the range of the data
        x.domain([200, 2400]);
        y.domain([0, 600]);

        // Add the valueline path.
        svg.append("path")
            .attr("class", "line")
            .attr("d", valueline(data_original))
            .attr("stroke", "red");

        // Add the valueline path.
        svg.append("path")
            .attr("class", "line")
            .attr("d", valueline(data_accelerated))
            .attr("stroke", "green");

        // Add the markers
        svg.selectAll('circle.mark.orig').data(data_original).enter().append('svg:circle')
            .attr('class', 'mark')
            .attr('cx', function(d){return x(d.size);})
            .attr('cy', function(d){return y(d.duration);})
            .attr('fill', 'red')
            .attr('r', 3.5);

        svg.selectAll('circle.mark.accel').data(data_accelerated).enter().append('svg:circle')
            .attr('class', 'mark')
            .attr('cx', function(d){return x(d.size);})
            .attr('cy', function(d){return y(d.duration);})
            .attr('fill', 'green')
            .attr('r', 3.5);

        // Add the X Axis
        svg.append("g")
            .attr("class", "x axis")
            .attr("transform", "translate(0," + height + ")")
            .call(xAxis);

        // Add the Y Axis
        svg.append("g")
            .attr("class", "y axis")
            .call(yAxis);

    //});

}
*/



/* STAR FIELD GENERATION */

// generate star fields
/*
generate_star_field(400, 1, 'far');
generate_star_field(160, 2, 'medium');
generate_star_field(30, 4, 'nearby');
generate_lightspeed_field(8);
*/

function generate_star_field(population, radius, container) {

        ['a', 'b'].forEach(function(k) {

            var starfield = $('.starfield.' + container + '.' + k);
            var height = $('#introduction').outerHeight() + 60;
            var width = $('#introduction').width();
            starfield.css('height', height);
            starfield.css('width', width);
            for (var i = 0; i < population; i++) {
                var cx = parseInt(Math.random() * width);
                var cy = parseInt(Math.random() * height);
                var star = $(document.createElementNS('http://www.w3.org/2000/svg', 'circle')).attr({
                    r: radius,
                    cx: cx,
                    cy: cy,
                    fill: '#fff'
                });
                starfield.append(star);
            }

        });
}

function generate_lightspeed_field(population) {


    ['a', 'b'].forEach(function(k) {

        var starfield = $('.lightspeed-starfield.' + k);
        var height = $('#introduction').outerHeight() + 60;
        var width = $('#introduction').width();
        starfield.css('height', height);
        starfield.css('width', width);

        var rectangles = starfield.children();
        var population_diff = population - rectangles.length;

        if (population_diff < 0) {

            var to_remove = rectangles.slice(0, -population_diff);
            $(to_remove).remove();

        } else {

            for (var i = 0; i < population_diff; i++) {
                var x = parseInt(Math.random() * width);
                var y = parseInt(Math.random() * height);
                var w = parseInt(Math.random() * 20 + 60);
                var h = parseInt(Math.random() * 2 + 1);
                var streak = $(document.createElementNS('http://www.w3.org/2000/svg', 'rect')).attr({
                    width: w,
                    height: h,
                    x: x,
                    y: y,
                    fill: '#fff'
                });
                starfield.append(streak);
            }

        }

    });
}



</script>

</body>

</html>
